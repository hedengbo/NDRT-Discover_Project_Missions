# NDRT-Discover_Project_Missions
This is the python code for an in-vehicle non-driving-related task (NDRT), named as discovery project mission.
The codes are based on Pytyhon 2.7. If newer version of the Python is used, you will need to update corresponding modules when necessary.

- The RUNMME Sce1.py is the main code to start the program.
- The largeAnsSet.txt is the file containing the correct answer for each phrase.
- The largeSet.txt is the file containing all candiate phrases that are used in the program.
- The RecievingTest.py and SendingTest.py can be used to test the connections between the sender and the receiver computers.

The task is created by Prof. Birsen Donmez (https://hfast.mie.utoronto.ca), updated by Dr. Dengbo He (https://personal.hkust-gz.edu.cn/hedengbo/index.html) 
during his Ph.D. study at the University of Toronto, under the guidance of Prof. Donmez.

Selected publications that are based on this NDRT include:

- He, D., Kanaan, D., & Donmez, B. (2021). In-vehicle displays to support driver anticipation of traffic conflicts in automated vehicles. Accident Analysis & Prevention, 149, 105842.
- He, D., & Donmez, B. (2022). The influence of visual-manual distractions on anticipatory driving. Human Factors, 64(2), 401-417.
- He, D., & Donmez, B. (2019). Influence of driving experience on distraction engagement in automated vehicles. Transportation Research Record, 2673(9), 142-151.
- He, D., DeGuzman, C. A., & Donmez, B. (2023). Anticipatory driving in automated vehicles: The effects of driving experience and distraction. Human Factors, 65(4), 663-663.
- Donmez, B., Boyle, L. N., & Lee, J. D. (2007). Safety implications of providing real-time feedback to distracted drivers. Accident Analysis & Prevention, 39(3), 581-590.
  
A description of the task:

This task was developed by Donmez et al. (2007) and has been shown across several studies to degrade driving performance (e.g., Chen et al., 2018; Merrikhpour & Donmez, 2017). Participants scrolled through 10 three-word phrases that looked similar to each other and had to find a phrase that had either “Discover” as its first word, or “Project” as its second word, or “Missions” as its third word (e.g., “Project Discover Misguide” is not a match, whereas “Discover Missions Predict” is). Only two phrases were visible on the screen at a time; participants used up and down arrows to scroll through the 10 phrases. Once participants identified a matching phrase, they had to tap on it and then tap on the submit button. Visual feedback was provided on the correctness of the submission, and then a “start” button appeared on the screen for the participants to initiate a new task. The task was available throughout the whole drive for the secondary task condition, and the participants could decide when to engage in the task and perform it at their own pace. It should be noted that this task is not purely visual-manual. The task is also cognitively demanding to some extent, as participants are required 
to recall the target phrase and compare it with the ones on the screen.

- Please cite this code as:

He D. & Donmez B. (2023). Dicovery Project Missions: A Visual-Manual Secondary Task. 
